---
title: "Weight-lifting Exercise"
author: "Jane Nyandele"
date: "2023-07-21"
output: html_document
---

# Introduction
With the rise of wearable devices like Jawbone Up, Nike FuelBand, and Fitbit, gathering extensive personal activity data has become affordable. These devices are part of the quantified self movement, where individuals regularly measure themselves to enhance health, identify behavioral patterns, or for tech enthusiasts. However, while people often quantify their activity levels, they rarely assess their performance quality. In this project, the aim is to use accelerometer data from the belt, forearm, arm, and dumbbell of six participants. These individuals were asked to perform Unilateral Dumbbell Biceps Curls in five ways: correctly (Class A) and with common mistakes, including throwing elbows forward (Class B), lifting the dumbbell halfway (Class C), lowering halfway (Class D), and throwing hips forward (Class E). The goal is to predict the class of each exercise using other predictors. This project is part of Coursera's Practical Machine Learning Week 4 - Peer-graded Assignment: Prediction Assignment Writeup.

## Data Source
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>


## Setting the stage and loading the necessary packages
```{r}

training.file   <- './data/pml-training.csv'
test.cases.file <- './data/pml-testing.csv'
training.url    <- 'http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
test.cases.url  <- 'http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'

#Directories
if (!file.exists("data")){
  dir.create("data")
}
if (!file.exists("data/submission")){
  dir.create("data/submission")
}

#R-Packages
install.packages("caret")
install.packages("tibble")
library(tibble)
library(ggplot2)
IscaretInstalled <- require("caret")
library(lattice)
install.packages("randomForest")
library(randomForest)
library(rpart)
library(rpart.plot)

```


## Loading and cleaning the datasets
In this section, we download and process the data, performing basic transformations and cleanup to remove NA values. We also remove irrelevant columns (columns 1 to 7) from the subset.

We use the pml-training.csv data for training and testing sets, while the pml-test.csv data is utilized to predict and answer the 20 questions using the trained model.
```{r}
# Download data
download.file(training.url, training.file)
download.file(test.cases.url,test.cases.file )

# Clean data
training   <-read.csv(training.file, na.strings=c("NA","#DIV/0!", ""))
testing <-read.csv(test.cases.file , na.strings=c("NA", "#DIV/0!", ""))
training<-training[,colSums(is.na(training)) == 0]
testing <-testing[,colSums(is.na(testing)) == 0]

# Subset data
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]
```


## Cross validation
In this section, we will conduct cross-validation by dividing the training data into 75% for training and 25% for testing.
```{r}
subSamples <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
subTraining <- training[subSamples, ] 
subTesting <- training[-subSamples, ]
```

## Expected out-of-sample error
The expected out-of-sample error is calculated as 1 minus the accuracy in the cross-validation data. Accuracy represents the proportion of correctly classified observations over the total sample in the subTesting dataset. The expected accuracy in the out-of-sample data (original testing dataset) is the anticipated accuracy level. Therefore, the expected out-of-sample error corresponds to the expected number of misclassified observations over the total observations in the Test dataset, which is the quantity: 1 minus accuracy from the cross-validation dataset.

### Exploratory analysis
The variable classe contains 5 levels. The plot of the outcome variable shows the frequency of each levels in the subTraining data.
```{r}
plot(subTraining$classe, col="lightblue", main="Levels of the variable classe", 
     xlab="classe levels", ylab="Frequency")

subTraining <- subTraining[!is.na(subTraining$classe), ]
subTraining$classe <- as.factor(subTraining$classe)
plot(subTraining$classe, col = "lightblue", main = "Levels of the variable classe", 
     xlab = "classe levels", ylab = "Frequency")
```
The plot above shows that Level A is the most frequent classe. D appears to be the least frequent one.


## Prediction models
We will then apply decision tree and random forest to the data in this section

### Decision Tree
```{r}
# Fit model
modFitDT <- rpart(classe ~ ., data=subTraining, method="class")

# Perform prediction
predictDT <- predict(modFitDT, subTesting, type = "class")

# Plot result
rpart.plot(modFitDT, main="Classification Tree", extra=102, under=TRUE, faclen=0)
```

### Random Forest
```{r}
# Fit model
modFitRF <- randomForest(classe ~ ., data=subTraining, method="class")

# Perform prediction
predictRF <- predict(modFitRF, subTesting, type = "class")
```


### Submission
In this section the files for the project submission are generated using the random forest algorithm on the testing data.
```{r}
# Perform prediction
predictSubmission <- predict(modFitRF, testing, type="class")
predictSubmission
```

```{r}
# Write files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./data/submission/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictSubmission)
```

## Conclusion

### Result:
The confusion matrices demonstrate that the Random Forest algorithm outperforms decision trees. The accuracy for the Random Forest model was 0.995 (95% CI: 0.993, 0.997) compared to 0.739 (95% CI: 0.727, 0.752) for the Decision Tree model. Hence, the Random Forest model is selected.

### Expected Out-of-Sample Error:
The estimated expected out-of-sample error is 0.005, or 0.5%. It is calculated as 1 minus the accuracy for predictions made against the cross-validation set. Since the cross-validation accuracy is above 99%, we can expect very few, if any, misclassified test samples from our Test dataset, which comprises 20 cases.


































































